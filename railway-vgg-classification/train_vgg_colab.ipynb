{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0787df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional - to save model directly to Drive)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pillow numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset zip file\n",
    "# Option 1: Upload manually via Files panel on left\n",
    "# Option 2: Use this code\n",
    "from google.colab import files\n",
    "print(\"Please upload railway_defect_dataset.zip\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Unzip dataset\n",
    "with zipfile.ZipFile('railway_defect_dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/')\n",
    "\n",
    "# Verify structure\n",
    "!ls -la /content/railway_defect_dataset/train/\n",
    "!ls -la /content/railway_defect_dataset/validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Dataset paths\n",
    "DATASET_DIR = Path('/content/railway_defect_dataset')\n",
    "TRAIN_DIR = DATASET_DIR / 'train'\n",
    "VAL_DIR = DATASET_DIR / 'validation'\n",
    "\n",
    "# Model configuration\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64  # Increased for GPU (was 32 on CPU)\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path('/content/railway_defect_output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f259c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable mixed precision for faster training\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print('‚úÖ Mixed precision enabled (2x faster training)')\n",
    "print(f'   Compute dtype: {policy.compute_dtype}')\n",
    "print(f'   Variable dtype: {policy.variable_dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3150857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators with augmentation\n",
    "print(\"üì¶ Creating Data Generators...\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded:\")\n",
    "print(f\"   Training samples: {train_generator.samples}\")\n",
    "print(f\"   Validation samples: {val_generator.samples}\")\n",
    "print(f\"   Classes: {list(train_generator.class_indices.keys())}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Save class indices\n",
    "with open(OUTPUT_DIR / 'class_indices.json', 'w') as f:\n",
    "    json.dump(train_generator.class_indices, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61726ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalanced dataset\n",
    "print(\"‚öñÔ∏è  Calculating class weights...\")\n",
    "\n",
    "# Count samples per class\n",
    "class_counts = {}\n",
    "for class_name in train_generator.class_indices.keys():\n",
    "    class_path = TRAIN_DIR / class_name\n",
    "    count = len(list(class_path.glob('*.jpg'))) + len(list(class_path.glob('*.png')))\n",
    "    class_counts[class_name] = count\n",
    "\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "for class_name, count in sorted(class_counts.items(), key=lambda x: -x[1]):\n",
    "    percentage = (count / total) * 100\n",
    "    print(f\"  {class_name:10s}: {count:4d} ({percentage:5.2f}%)\")\n",
    "\n",
    "# Compute class weights\n",
    "class_names = sorted(train_generator.class_indices.keys(), key=lambda x: train_generator.class_indices[x])\n",
    "class_counts_array = [class_counts[name] for name in class_names]\n",
    "weights = compute_class_weight('balanced', classes=np.arange(len(class_names)), y=np.repeat(np.arange(len(class_names)), class_counts_array))\n",
    "class_weight_dict = {i: w for i, w in enumerate(weights)}\n",
    "\n",
    "print(\"\\nCalculated class weights:\")\n",
    "for class_name, class_idx in sorted(train_generator.class_indices.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {class_name:10s}: {class_weight_dict[class_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaad151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build VGG16 model\n",
    "print(\"üèóÔ∏è  Building VGG16 model...\")\n",
    "\n",
    "# Load pre-trained VGG16\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build classification head\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax', dtype='float32')  # Force float32 for stability\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "print(\"\\n‚úÖ Model built successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    str(OUTPUT_DIR / 'best_model_initial.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stop, reduce_lr]\n",
    "print(\"‚úÖ Callbacks configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase 1: Frozen base model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Starting Training - Phase 1: Frozen Base Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 1 training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history - Phase 1\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history_phase1.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_phase1.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Phase 1: Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history_phase1.history['loss'], label='Train Loss')\n",
    "plt.plot(history_phase1.history['val_loss'], label='Val Loss')\n",
    "plt.title('Phase 1: Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history_phase1.history['precision'], label='Train Precision')\n",
    "plt.plot(history_phase1.history['recall'], label='Train Recall')\n",
    "plt.title('Phase 1: Precision & Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_history_phase1.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Phase 2: Unfreeze top layers\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Starting Training - Phase 2: Fine-tuning\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Unfreeze last 4 layers of VGG16\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Trainable layers: {sum([1 for layer in model.layers if layer.trainable])}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE / 10),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Update checkpoint path\n",
    "checkpoint_finetune = ModelCheckpoint(\n",
    "    str(OUTPUT_DIR / 'best_model_finetuned.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_finetune = [checkpoint_finetune, early_stop, reduce_lr]\n",
    "\n",
    "# Fine-tune\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=30,  # Fewer epochs for fine-tuning\n",
    "    callbacks=callbacks_finetune,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 2 fine-tuning completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history - Phase 2\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history_phase2.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_phase2.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Phase 2: Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history_phase2.history['loss'], label='Train Loss')\n",
    "plt.plot(history_phase2.history['val_loss'], label='Val Loss')\n",
    "plt.title('Phase 2: Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history_phase2.history['precision'], label='Train Precision')\n",
    "plt.plot(history_phase2.history['recall'], label='Train Recall')\n",
    "plt.title('Phase 2: Precision & Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_history_phase2.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model\n",
    "print(\"\\nüìä Final Model Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model\n",
    "best_model = keras.models.load_model(OUTPUT_DIR / 'best_model_finetuned.keras')\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_loss, val_acc, val_precision, val_recall = best_model.evaluate(val_generator, verbose=1)\n",
    "\n",
    "print(f\"\\n‚úÖ Final Results:\")\n",
    "print(f\"   Validation Accuracy:  {val_acc*100:.2f}%\")\n",
    "print(f\"   Validation Loss:      {val_loss:.4f}\")\n",
    "print(f\"   Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"   Validation Recall:    {val_recall:.4f}\")\n",
    "print(f\"   F1-Score:             {2 * (val_precision * val_recall) / (val_precision + val_recall):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained model\n",
    "print(\"\\nüì• Download your trained model:\")\n",
    "from google.colab import files\n",
    "\n",
    "# Zip the output directory\n",
    "!zip -r railway_defect_output.zip /content/railway_defect_output/\n",
    "\n",
    "# Download\n",
    "files.download('railway_defect_output.zip')\n",
    "print(\"\\n‚úÖ Model downloaded! Extract and copy best_model_finetuned.keras to your mobile_backend folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c5139",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "1. **Download** the `railway_defect_output.zip` file\n",
    "2. **Extract** and copy `best_model_finetuned.keras` to your project\n",
    "3. **Update** your mobile_backend to use the new model:\n",
    "   ```bash\n",
    "   cp best_model_finetuned.keras ~/Desktop/Code_on_track/mobile_backend/best_model.keras\n",
    "   ```\n",
    "4. **Test** the model with your rust images\n",
    "\n",
    "## Performance Comparison:\n",
    "- **MacBook Air M1**: ~30 hours (CPU only)\n",
    "- **Google Colab T4 GPU**: ~2-3 hours ‚ö°Ô∏è\n",
    "- **Speed increase**: ~10-15x faster!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
